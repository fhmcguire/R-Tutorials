---
title: "Training Module 2: Introduction to Bayesian Statistics with `brms`"
author: "Hunter McGuire"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 4
    code_folding: show
    theme: spacelab
    highlight: tango
  pdf_document:
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Purpose

This training module introduces the foundational concepts and practical tools of Bayesian statistics, with an emphasis on using R for applied data analysis.

By the end of this module, you will be able to:

*   Understand the core components of Bayesian inference: prior, likelihood, posterior, and Bayes' Theorem.
*   Distinguish between Bayesian and frequentist approaches, including the interpretation of credible intervals.
*   Use R (especially the `brms` package) to fit Bayesian models and interpret posterior distributions.
*   Apply Bayesian methods to analyze data, including setting informative or weakly informative priors, generating posterior predictions, and assessing model diagnostics using MCMC.

Prior to starting, we will load the data we generated in the `SAE-Intro-R.Rmd` module (`data_for_training_modules.rds`) and the following R packages:

*   `brms`: Bayesian regression modeling in Stan
*   `dplyr`: Data management/processing tasks
*   `tidyr`: Data management/processing tasks
*   `ggplot2`: General data visualizations
*   `bayesplot`: Data visualizations of Bayesian regression model convergence diagnostics

**Note:** If necessary, you can install packages, like `brms`, using the code `install.packages("brms")`. You only need to run this installation function once on your computer.

```{r, warning=FALSE, message=FALSE}
# load packages
library(brms)
library(dplyr)
library(tidyr)
library(ggplot2)
library(bayesplot)

# load the data
dta <- readRDS(file = "data_for_training_modules.rds")
```


## Key concepts in Bayesian statistics

In this section, we’ll break down the core ideas that make Bayesian statistics distinct from traditional (frequentist) approaches. These concepts form the foundation for Bayesian modeling.

### Prior

A prior expresses what we believe about a parameter before seeing the data. In `brms` and other Bayesian modeling software, you can set priors explicitly.

```{r}
# set a normal prior on the beta coefficients (class = "b")
# mean = 0; standard deviation (sd) = 5
prior_example <- 
  set_prior("normal(0,5)",
            class = "b")
```

This says: "Before seeing the data, I believe the beta coefficients are around 0 (null value) with a good amount of uncertainty (sd = 5) around this value (i.e., betas could potentially be much higher or lower than 0)."

We can also visualize our prior distribution assumptions by using the `ggplot2` package.

```{r}
# Create a sequence of values to represent possible beta coefficients (e.g., from -20 to 20)
x <- seq(from = -20, 
         to = 20, 
         length.out = 1000)

# Set the prior distribution (normal with mean 0 and sd 5)
prior_values <- dnorm(x, 
                      mean = 0, 
                      sd = 5)

# Create a data frame for ggplot
prior_df <- data.frame(x = x, 
                       y = prior_values)

# Plot the prior distribution
ggplot(prior_df, aes(x = x, 
                     y = y)) +
  geom_line(linewidth = 2,
            color = "orange3") + 
  labs(title = "Normal Prior Distribution for Beta Coefficients",
       subtitle = "A normal distribution with mean = 0 and sd = 5",
       x = "Beta Coefficients (theta)",
       y = "Density") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5))
```


### Likelihood

The likelihood represents how probable the observed data is, given a model. In practice, you choose a likelihood function through your model family (e.g., Normal (Gaussian), Bernoulli, Poisson, etc.).


### Posterior
The posterior is t
he updated distribution for the parameters after combining the prior and the data. Each row is one draw from the posterior — a plausible value of the parameters given the data and the prior.

### Bayes' Theorem

Bayes' Theorem ties the prior, likelihood, and posterior together:

$$
Posterior \propto Likelihood \times Prior
$$
More specifically,

$$
P(\text{belief | data}) = \frac{P(\text{data} \mid \text{belief}) \times P(\text{belief})}{P(\text{data})}
$$
This means that the posterior probability of a belief, given the observed data, is equal to the likelihood of the data under that belief, multiplied by the prior probability of the belief, and divided by the marginal probability of the data (which serves as a normalizing constant). In the above equation, $P(\text{belief | data})$ is the posterior probability, $P(\text{data} \mid \text{belief)}$ is the likelihood, $P(\text{belief})$ is the prior probability, and $P(\text{data})$ is the normalizing constant.

In R, you don’t need to code this directly since `brms` handles the math. But you can simulate the idea. This code simulates how the posterior is shaped by combining prior and data.

```{r}
# Simulate prior, likelihood, and compute the posterior manually

# Define the parameter grid
theta <- seq(-1, 4, 
             length.out = 1000)
dx <- theta[2] - theta[1]  # grid spacing

# Define prior: Normal(1, 1)
prior <- dnorm(theta, 
               mean = 1, 
               sd = 1)

# Define likelihood: Normal(2, 0.5)
likelihood <- dnorm(theta, 
                    mean = 2, 
                    sd = 0.5)

# Compute unnormalized posterior (Bayes Rule: posterior ∝ prior × likelihood)
posterior_unnorm <- prior * likelihood

# Normalize the posterior to integrate to 1
posterior <- posterior_unnorm / sum(posterior_unnorm * dx)

# check that the total area under the posterior density sums to 1
print(sum(posterior * dx))
```

Now we can visualize the prior, likelihood, and posterior using the `ggplot2` package.

```{r}
# Create a data frame for plotting
plot_df <- data.frame(
  theta = theta,
  Prior = prior,
  Likelihood = likelihood,
  Posterior = posterior
)

# Convert to long format for ggplot2
library(tidyr)
plot_df_long <- pivot_longer(plot_df, 
                             cols = c(Prior, Likelihood, Posterior),
                             names_to = "Distribution", 
                             values_to = "Density")

# Plot all distributions
ggplot(plot_df_long, 
       aes(x = theta,
           y = Density,
           color = Distribution)) +
  geom_line(linewidth = 1.2) +
  theme_minimal(base_size = 16) +
  labs(title = "Bayesian Updating: Prior, Likelihood, Posterior",
       x = expression(theta),
       y = "Density") +
  scale_color_manual(values = c("Prior" = "steelblue", 
                                "Likelihood" = "orange", 
                                "Posterior" = "green4"))
```

### Credible intervals

Lastly, we'll discuss the difference between **confidence intervals** (used in frequentist statistics) and **credible intervals**. Credible intervals are generally much easier to interpret.

*   A **95% confidence interval** means: "If we repeated this experiment many times, 95% of the calculated intervals would contain the true parameter value." Confidence intervals rely on the long-run frequency of intervals and not the probability for a specific interval.
*   A **95% credible interval**, on the other hand, means: "Given the data we have and our prior information, there's a 95% probability that the parameter lies within this interval." In contrast to confidence intervals, this is a direct probability statement about the parameter.

For example, let's extract a 95% credible interval from the posterior we generated above:

```{r}
# Calculate a 95% credible interval using the quantile() function
credible_interval <- plot_df_long %>%
  # limit the dataset to the Posterior values
  dplyr::filter(Distribution == "Posterior") %>%
  # pull out the theta values of the posterior as a vector
  dplyr::pull(theta) %>%
  # compute the 95% interval
  quantile(.,
           probs = c(0.025, 0.975)) %>%
  # round the values to 2 digits after the decimal 
  round(., 2)

# display the credible interval
credible_interval
```

This shows that the 95% of the posterior values of $\theta$ range from `r credible_interval[1]` to `r credible_interval[2]`.

## Application

To show how this works, we will predict a continouous outcome variable (`health_score_revised` or Y) using a single categorical predictor variable (`education` or X).

First, let's examine the distributions of the `health_score_revised` and `education` variables using the `table()` (for categorical) and `summary()` (for continuous) functions.

```{r}
# Output the frequencies of each education level
table(dta$education)

# Output the summary values of the health_score_revised variable
summary(dta$health_score_revised)

# Examine how the health scores may differ across education levels
dta %>%
  group_by(education) %>%
  summarize(n = n(),
            mean_health_score = mean(health_score_revised, na.rm=TRUE))
```

This shows that as education level rises, the mean value of the health scores tend to rise.

Now let's use the `brm()` function from the `brms` package to fit a Bayesian regression model.

```{r, eval=FALSE}
# Since bayesian modeling involves stochastic (random) processes, you should set
# a random seed prior to fitting your model. This ensures that your results
# will be reproducible.
set.seed(28752)

# Use education level to predict health score
# Note: Education
# Note: "1 +" explicitly adds the model intercept (this is automatically included)
fit <- brm(health_score_revised ~ 1 + education, 
           # use the simulated data we created in "SAE-Intro-R.Rmd"
           data = dta, 
           # use the prior we specified above
           prior = prior_example, 
           # Use a Gaussian (Normal) outcome distribution with the identity link function.
           family = gaussian(link="identity"),
           # use the default brms specifications for the number of MCMC chains,
           # iterations per chain, warmups per chain, and thinning.
           chains = 4,
           iter = 2000,
           warmup = 1000,
           thin = 1)

# We can save this R object to our working directory so we don't have to run it again.
saveRDS(fit, file = "fit_intro_bayesian.rds")
```

Now, let's examine the results by calling the `summary()` function.

```{r}
# load the `brms` fit object from our working directory
fit <- readRDS(file = "fit_intro_bayesian.rds")

summary(fit)
```

This shows that our intercept value (i.e., the predicted value of health score for the referent education group [High School]) is `r round(brms::fixef(fit)[1,1],2)`. Those with some college education have a slightly lower health score (`r round(brms::fixef(fit)[1,1] + brms::fixef(fit)[2,1],2)`), while those with a Bachelor's degree (`r round(brms::fixef(fit)[1,1] + brms::fixef(fit)[3,1],2)`) or a Graduate degree (`r round(brms::fixef(fit)[1,1] + brms::fixef(fit)[4,1],2)`) have higher health scores relative to those with a High School degree.

## Model convergence diagnostics

After fitting a Bayesian model, it is critical to assess whether the Markov chains have converged and whether we have a sufficient number of effective samples to trust the posterior estimates. Here, we examine four common diagnostics:

*   Gelman Rubin R-hat values
*   Effective sample size (ESS)
*   Trace plots
*   Autocorrelation plots

### Gelman Rubin R-hat values

R-hat (or $\hat{R}$) is a convergence diagnostic that compares the variance within each chain to the variance between chains. If all chains have converged to the same target distribution, the R-hat value should be close to 1.

*   **Interpretation:** R-hat values should ideally be below 1.05, where values close to 1.00 are preferred. Values above 1.05 suggest that chains may not have fully converged and more iterations or model adjustments might be needed.

```{r}
summary(fit)$fixed[, "Rhat"]
```

### Effective sample size (ESS)

Effective Sample Size (ESS) quantifies how many independent draws we effectively have from the posterior distribution, accounting for autocorrelation in the Markov chains. It helps assess the quality and reliability of parameter estimates.

In `brms` (via `Stan`), even though we may have a large number of post-warmup iterations, autocorrelation within chains means that not all samples are fully independent. ESS gives us a corrected number that reflects this.

In our example, we ran 4 chains with 2,000 iterations each and a 1,000-iteration warmup. That gives us:

*   4 chains * 1,000 post-warmup iterations = 4,000 total draws

However, if there is substantial autocorrelation, the effective sample size might only be 500 or 1,000, indicating that we have fewer effectively independent samples than it appears.

There are two types of effective sample size: 
*   Bulk ESS assesses sampling efficiency in the main body of the posterior.
*   Tail ESS assesses efficiency in the tails of the distribution, which is important for accurate uncertainty estimates and extreme quantiles.

*   **Interpretation:** Higher ESS values are better. For stable estimates, aim for ESS > 400. Low ESS may indicate high autocorrelation or poor mixing of chains. In this example, the ESS figures are quite high and close to the number of our total draws (4,000), which shows that our model has performed well with minimal autocorrelation between draws.

```{r}
summary(fit)$fixed[, c("Bulk_ESS", "Tail_ESS")]
```


### Trace plots

Trace plots show the sampled values of each parameter across iterations and chains. These plots help assess whether chains are well mixed and have converged to a common distribution.

*   **Interpretation:** The chains should look like "fuzzy caterpillars" with no drift or trend, and all chains should appear to be overlapping and exploring the same area of the posterior. In this example, we see the "fuzzy caterpillars", which gives further evidence that the 4 Markov chains have converged on the same parameter space for each beta coefficient, respectively.

```{r}
bayesplot::mcmc_trace(as.array(fit),
                      pars = c("b_Intercept", "b_educationSomeCollege",
                               "b_educationBachelors", "b_educationGraduate"))
```

### Autocorrelation plots

Autocorrelation plots display the correlation between samples that are a certain number of steps (lags) apart. High autocorrelation can reduce the effective sample size and signal poor mixing.

*   **Interpretation:** We want low autocorrelation, especially at higher lags. High autocorrelation (bars that stay high across lags) suggests the chains are moving slowly through the posterior and may need thinning or more iterations. In the example below, the autocorrelation values quickly drop down to near zero, which is ideal for model convergence.

```{r}
bayesplot::mcmc_acf(fit,
                    pars = c("b_Intercept", "b_educationSomeCollege"))

bayesplot::mcmc_acf(fit,
                    pars = c("b_educationBachelors", "b_educationGraduate"))
```


